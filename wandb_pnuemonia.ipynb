{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import classification_report\n",
    "import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import numpy as np\n",
    "import wandb\n",
    "# 환경 변수 설정: CUDA 비동기 오류 디버깅을 위해 설정\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 클래스 정의\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        normal_dir = os.path.join(data_dir, 'NORMAL')\n",
    "        for img_name in os.listdir(normal_dir):\n",
    "            self.image_paths.append(os.path.join(normal_dir, img_name))\n",
    "            self.labels.append(0)\n",
    "\n",
    "        pneumonia_dir = os.path.join(data_dir, 'PNEUMONIA')\n",
    "        for img_name in os.listdir(pneumonia_dir):\n",
    "            self.image_paths.append(os.path.join(pneumonia_dir, img_name))\n",
    "            self.labels.append(1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert('L')\n",
    "        label = self.labels[idx]\n",
    "        return np.array(image), label\n",
    "    \n",
    "# transform을 적용한 새로운 데이터셋 클래스 정의\n",
    "class TransformedDataset(Dataset):\n",
    "    def __init__(self, dataset, indices, transform=None):\n",
    "        self.dataset = Subset(dataset, indices)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.dataset[idx]\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의\n",
    "class EnhancedCNN(nn.Module): \n",
    "    def __init__(self): \n",
    "        super(EnhancedCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(128*28*28, 256)\n",
    "        self.fc2 = nn.Linear(256, 2)\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = x.view(-1, 128*28*28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습 함수 정의\n",
    "def train():\n",
    "    # wandb 설정 초기화\n",
    "    wandb.init()\n",
    "    config = wandb.config  # 하이퍼파라미터 설정\n",
    "\n",
    "    # 데이터셋 로드 및 전처리\n",
    "    full_dataset = CustomDataset(data_dir='chest_xray/train')\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "    train_indices, val_indices = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "    train_transform = A.Compose([\n",
    "        A.Resize(224, 224),\n",
    "        A.Rotate(limit=5),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "        A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=0, p=0.5),\n",
    "        A.GaussNoise(var_limit=(10.0, 50.0), p=0.2),\n",
    "        A.Normalize(mean=(0.5,), std=(0.5,)),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "    val_transform = A.Compose([\n",
    "        A.Resize(224, 224),\n",
    "        A.Normalize(mean=(0.5,), std=(0.5,)),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "    train_dataset = TransformedDataset(full_dataset, train_indices.indices, transform=train_transform)\n",
    "    val_dataset = TransformedDataset(full_dataset, val_indices.indices, transform=val_transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "    # 모델, 손실 함수, 옵티마이저 설정\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = EnhancedCNN().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    early_stop_counter = 0\n",
    "    patience = 7\n",
    "\n",
    "    for epoch in range(config.epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        all_train_labels = []\n",
    "        all_train_predictions = []\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for images, labels in tqdm.tqdm(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_train_labels.extend(labels.cpu().numpy())\n",
    "            all_train_predictions.extend(predicted.cpu().numpy())\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        train_accuracy = 100 * correct / total\n",
    "        train_report = classification_report(all_train_labels, all_train_predictions, target_names=['NORMAL', 'PNEUMONIA'], output_dict=True)\n",
    "        train_recall = train_report['weighted avg']['recall']\n",
    "        print(f'Epoch [{epoch+1}/{config.epochs}] - Loss: {running_loss / len(train_loader):.4f}, Train Accuracy: {train_accuracy:.2f}%, recall: {train_recall:.4f}')\n",
    "\n",
    "        wandb.log({\"Train Loss\": running_loss / len(train_loader), \"Train Accuracy\": train_accuracy, \"Train Recall\": train_recall})\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_val_labels = []\n",
    "        all_val_predictions = []\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                all_val_labels.extend(labels.cpu().numpy())\n",
    "                all_val_predictions.extend(predicted.cpu().numpy())\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_accuracy = 100 * correct / total\n",
    "        val_report = classification_report(all_val_labels, all_val_predictions, target_names=['NORMAL', 'PNEUMONIA'], output_dict=True)\n",
    "        val_recall = val_report['weighted avg']['recall']\n",
    "        print(f'Validation Loss: {val_loss / len(val_loader):.4f}, Valid Accuracy: {val_accuracy:.2f}%, recall: {val_recall:.4f}')\n",
    "\n",
    "        wandb.log({\"Validation Loss\": val_loss / len(val_loader), \"Validation Accuracy\": val_accuracy, \"Validation Recall\": val_recall})\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            if early_stop_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    # 테스트 데이터셋 준비\n",
    "    test_transform = A.Compose([\n",
    "        A.Resize(224, 224),\n",
    "        A.Normalize(mean=(0.5,), std=(0.5,)),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "    test_dataset = CustomDataset(data_dir='chest_xray/test')\n",
    "    test_dataset = TransformedDataset(test_dataset, range(len(test_dataset)), transform=test_transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "    # 모델 평가 모드로 설정\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Classification Report 출력 및 Weighted Average Recall 추출\n",
    "    report = classification_report(all_labels, all_predictions, target_names=['NORMAL', 'PNEUMONIA'], output_dict=True)\n",
    "    weighted_avg_recall = report['weighted avg']['recall']\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(all_labels, all_predictions, target_names=['NORMAL', 'PNEUMONIA']))\n",
    "    print(f\"Weighted Average Recall: {weighted_avg_recall:.4f}\")\n",
    "\n",
    "    # 정확도 출력\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy: {accuracy:.4f}%')\n",
    "\n",
    "    # wandb에 최종 성능 로그\n",
    "    wandb.log({\"Test Accuracy\": accuracy, \"Test Weighted Avg Recall\": weighted_avg_recall})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb sweep 설정\n",
    "sweep_config = {\n",
    "    'method': 'bayes',  # 하이퍼파라미터 검색 방법: grid, random, bayes\n",
    "    'metric': {'name': 'val_accuracy', 'goal': 'maximize'},  # 최적화할 메트릭\n",
    "    'parameters': {\n",
    "        'learning_rate': {'min': 0.0001, 'max': 0.01},  # 학습률 검색 범위\n",
    "        'batch_size': {'values': [8, 16, 32]},  # 배치 크기 후보\n",
    "        'epochs': {'values': [10, 20, 30]},  # 에포크 수 후보\n",
    "        'optimizer': {'values': ['adam', 'sgd']}  # 최적화 알고리즘 후보\n",
    "    },\n",
    "    # Early Stopping 설정 추가\n",
    "    'early_terminate': {\n",
    "        'type': 'hyperband',  # 조기 종료 방법\n",
    "        'min_iter': 5         # 최소 반복 수\n",
    "    },\n",
    "    # Sweep 이름 추가\n",
    "    'name': 'my-optimized-sweep-experiment'\n",
    "}\n",
    "\n",
    "# sweep 생성\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"pneumonia-detection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 에이전트 실행\n",
    "wandb.agent(sweep_id, function=train)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
