{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import classification_report\n",
    "import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "# GPU 디버깅을 위한 환경 변수 설정\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 클래스 정의\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        normal_dir = os.path.join(data_dir, 'NORMAL')\n",
    "        for img_name in os.listdir(normal_dir):\n",
    "            self.image_paths.append(os.path.join(normal_dir, img_name))\n",
    "            self.labels.append(0)\n",
    "\n",
    "        pneumonia_dir = os.path.join(data_dir, 'PNEUMONIA')\n",
    "        for img_name in os.listdir(pneumonia_dir):\n",
    "            self.image_paths.append(os.path.join(pneumonia_dir, img_name))\n",
    "            self.labels.append(1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert('L')\n",
    "        label = self.labels[idx]\n",
    "        return np.array(image), label\n",
    "    \n",
    "# transform을 적용한 새로운 데이터셋 클래스 정의\n",
    "class TransformedDataset(Dataset):\n",
    "    def __init__(self, dataset, indices, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.indices = indices\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        actual_idx = self.indices[idx]  # 전체 데이터셋에서의 실제 인덱스\n",
    "        image, label = self.dataset[actual_idx]\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "        return image, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 데이터 증강 변환 추가\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.Rotate(limit=5),  # 5도 이내로만 회전\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=5, p=0.5),  # 회전 범위 제한\n",
    "    A.GaussNoise(var_limit=(10.0, 50.0), p=0.2),\n",
    "    # A.ElasticTransform(p=0.3),\n",
    "    # A.GridDistortion(p=0.3),\n",
    "    A.CLAHE(p=0.3),\n",
    "    A.RandomResizedCrop(224, 224, scale=(0.8, 1.0), ratio=(0.75, 1.33), p=0.5),\n",
    "    A.Normalize(mean=(0.5,), std=(0.5,)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.Normalize(mean=(0.5,), std=(0.5,)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 데이터셋 로드 (transform 없음)\n",
    "full_dataset = CustomDataset(data_dir='chest_xray/train')\n",
    "\n",
    "# 데이터셋 분할 (80% 훈련, 20% 검증)\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_indices, val_indices = random_split(range(len(full_dataset)), [train_size, val_size])\n",
    "\n",
    "# 데이터 증강을 적용한 데이터셋 생성\n",
    "train_dataset = TransformedDataset(full_dataset, train_indices, transform=train_transform)\n",
    "val_dataset = TransformedDataset(full_dataset, val_indices, transform=val_transform)\n",
    "\n",
    "# 클래스 불균형을 고려한 샘플링 가중치 계산\n",
    "train_labels = [full_dataset.labels[i] for i in train_indices]  # train 데이터셋에 대한 레이블\n",
    "normal_count = train_labels.count(0)\n",
    "pneumonia_count = train_labels.count(1)\n",
    "class_weights = 1. / torch.tensor([normal_count, pneumonia_count], dtype=torch.float)\n",
    "sample_weights = [class_weights[label] for label in train_labels]\n",
    "sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "# DataLoader에 WeightedRandomSampler 추가\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, sampler=sampler, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImprovedCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        # Dropout 추가\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(256 * 14 * 14, 512)\n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
    "        x = x.view(-1, 256 * 14 * 14)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 초기화\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ImprovedCNN().to(device)\n",
    "\n",
    "# 옵티마이저와 스케줄러 설정\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)  # L2 정규화 추가\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min', \n",
    "    factor=0.5,    # 학습률을 조금 덜 줄이도록 변경\n",
    "    patience=7,    # 조금 더 기다린 후 학습률을 줄이도록 설정\n",
    "    threshold=1e-3, # 변화량에 민감하지 않도록 약간 상향\n",
    "    cooldown=2,    # 학습률 감소 후 2 에포크 동안은 다시 감소하지 않도록 설정\n",
    "    min_lr=1e-6    # 학습률의 하한선 설정\n",
    ")\n",
    "\n",
    "# 데이터셋 로드 후 클래스 불균형 계산\n",
    "normal_count = len([label for label in full_dataset.labels if label == 0])\n",
    "pneumonia_count = len([label for label in full_dataset.labels if label == 1])\n",
    "# 손실 함수 정의 (가중치 적용)\n",
    "class_weights = torch.tensor([pneumonia_count / normal_count, 1.0], device=device)\n",
    "\n",
    "# 손실 함수 정의 (가중치 적용)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "num_epochs = 30\n",
    "patience = 7\n",
    "best_val_loss = float('inf')\n",
    "early_stop_counter = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131/131 [00:20<00:00,  6.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30] - Loss: 2.3811, Train Accuracy: 64.93%, recall: 0.6493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.4341, Valid Accuracy: 76.15%, recall: 0.7615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 98/131 [00:16<00:05,  6.18it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_train_labels = []\n",
    "    all_train_predictions = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in tqdm.tqdm(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        all_train_labels.extend(labels.cpu().numpy())\n",
    "        all_train_predictions.extend(predicted.cpu().numpy())\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    train_accuracy = 100 * correct / total\n",
    "    train_report = classification_report(all_train_labels, all_train_predictions, target_names=['NORMAL', 'PNEUMONIA'], output_dict=True)\n",
    "    train_recall = train_report['weighted avg']['recall']\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}] - Loss: {running_loss / len(train_loader):.4f}, Train Accuracy: {train_accuracy:.2f}%, recall: {train_recall:.4f}')\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_val_labels = []\n",
    "    all_val_predictions = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_val_labels.extend(labels.cpu().numpy())\n",
    "            all_val_predictions.extend(predicted.cpu().numpy())\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_accuracy = 100 * correct / total\n",
    "    val_report = classification_report(all_val_labels, all_val_predictions, target_names=['NORMAL', 'PNEUMONIA'], output_dict=True)\n",
    "    val_recall = val_report['weighted avg']['recall']\n",
    "    print(f'Validation Loss: {val_loss / len(val_loader):.4f}, Valid Accuracy: {val_accuracy:.2f}%, recall: {val_recall:.4f}')\n",
    "\n",
    "    # 학습률 스케줄러 스텝\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # 조기 종료(Early Stopping) 체크\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        early_stop_counter = 0\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        if early_stop_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      NORMAL       0.96      0.46      0.62       242\n",
      "   PNEUMONIA       0.75      0.99      0.85       398\n",
      "\n",
      "    accuracy                           0.79       640\n",
      "   macro avg       0.85      0.72      0.74       640\n",
      "weighted avg       0.83      0.79      0.76       640\n",
      "\n",
      "Weighted Average Recall: 0.7875\n",
      "Accuracy: 78.7500%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 테스트 데이터셋 준비 및 평가\n",
    "test_transform = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.Normalize(mean=(0.5,), std=(0.5,)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "test_dataset = CustomDataset(data_dir='chest_xray/test')\n",
    "test_dataset = TransformedDataset(test_dataset, range(len(test_dataset)), transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "model.eval()\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "report = classification_report(all_labels, all_predictions, target_names=['NORMAL', 'PNEUMONIA'], output_dict=True)\n",
    "weighted_avg_recall = report['weighted avg']['recall']\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_labels, all_predictions, target_names=['NORMAL', 'PNEUMONIA']))\n",
    "print(f\"Weighted Average Recall: {weighted_avg_recall:.4f}\")\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy: {accuracy:.4f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "module01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
