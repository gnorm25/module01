{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import torchvision\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "class customdataset(Dataset): \n",
    "    def __init__(self, data_dir, transform= None): \n",
    "        #이닛 해주고\n",
    "        self.data_dir= data_dir \n",
    "        self.transform= transform\n",
    "        #이미지들 경로명 담을 리스트\n",
    "        self.image_paths= []\n",
    "        self.labels= []\n",
    "\n",
    "        #이제 여기서 데이터를 결정해줘야해. \n",
    "        #경로에서 'NORMAL' 폴더에 있는 하위 파일들을 모두 리스트로 만듦\n",
    "        # normal_dir = os.path.join(data_dir, 'NORMAL')\n",
    "        # for img in glob(f\"{normal_dir}/*\"): \n",
    "        #     self.image_paths.append(os.path.join(normal_dir, img))\n",
    "        #     self.labels.append(0)\n",
    "\n",
    "        # pneumonia_dir = os.path.join(data_dir, 'PNEUMONIA')\n",
    "        # for img in glob(f\"{pneumonia_dir}/*\"): \n",
    "        #     self.image_paths.append(os.path.join(pneumonia_dir, img))\n",
    "        #     self.labels.append(1)\n",
    "\n",
    "        # NORMAL 폴더의 이미지 경로와 라벨 (0)\n",
    "        normal_dir = os.path.join(data_dir, 'NORMAL')\n",
    "        for img_name in os.listdir(normal_dir):\n",
    "            self.image_paths.append(os.path.join(normal_dir, img_name))\n",
    "            self.labels.append(0)\n",
    "\n",
    "        # PNEUMONIA 폴더의 이미지 경로와 라벨 (1)\n",
    "        pneumonia_dir = os.path.join(data_dir, 'PNEUMONIA')\n",
    "        for img_name in os.listdir(pneumonia_dir):\n",
    "            self.image_paths.append(os.path.join(pneumonia_dir, img_name))\n",
    "            self.labels.append(1)\n",
    "\n",
    "            \n",
    "    def __len__(self): \n",
    "        #len이건 필수래 나중에 DataLoader 하려면 \n",
    "        #몇번 반복해서 받을건지를 결정하는 요소? \n",
    "        #이미지파일 갯수만큼 받아야하니까\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        #이것도 필수래 DataLoader 에서 이터레이터한 객체를 받아올 수 있다는데? \n",
    "        #그래서 for문같은 반복문 추가 안해줘도 DataLoader에서 알아서 반복 해준대 ㅇㅇ\n",
    "        #이미지 경로를 받아서 실제 이미지를 열어주는 구간\n",
    "        \n",
    "        image = Image.open(self.image_paths[idx]).convert('L')\n",
    "        label= self.labels[idx]\n",
    "\n",
    "        if self.transform: \n",
    "            image= self.transform(image)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform= transforms.Compose([ \n",
    "    #데이터 전처리 하기위해서 필수인 과정 \n",
    "    #데이터 증강도 할 수 있음 +a인 느낌 \n",
    "    transforms.Resize((224, 224)),\n",
    "    # transforms.Grayscale(num_output_channels=1), \n",
    "    transforms.RandomHorizontalFlip(),      # 수평 뒤집기\n",
    "    transforms.RandomRotation(10),          # 10도 이내로 회전\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),  # 밝기 및 대비 조절\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # 회전 없이 이동 변환만 적용\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize(mean=[0.5], std=[0.5]) \n",
    "])\n",
    "\n",
    "val_transform= transforms.Compose([\n",
    "    #검증 데이터셋을 만들 땐 전처리만\n",
    "    #데이터 증강은 필요없음. \n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Grayscale(num_output_channels=1), \n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize(mean=[0.5], std=[0.5]) \n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset= customdataset(data_dir= 'chest_xray/train', transform= train_transform) \n",
    "train_size= int(0.8*len(full_dataset)) \n",
    "val_size= len(full_dataset) - train_size\n",
    "train_dataset, val_dataset= random_split(full_dataset, [train_size, val_size]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5216"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
      "    RandomHorizontalFlip(p=0.5)\n",
      "    RandomRotation(degrees=[-10.0, 10.0], interpolation=nearest, expand=False, fill=0)\n",
      "    ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2), saturation=None, hue=None)\n",
      "    RandomAffine(degrees=[0.0, 0.0], translate=(0.1, 0.1))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.5], std=[0.5])\n",
      ") Compose(\n",
      "    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
      "    RandomHorizontalFlip(p=0.5)\n",
      "    RandomRotation(degrees=[-10.0, 10.0], interpolation=nearest, expand=False, fill=0)\n",
      "    ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2), saturation=None, hue=None)\n",
      "    RandomAffine(degrees=[0.0, 0.0], translate=(0.1, 0.1))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.5], std=[0.5])\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(full_dataset.transform, train_dataset.dataset.transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 검증 데이터는 데이터 증강을 적용하지 않기 때문에 변환을 따로 지정\n",
    "val_dataset.dataset = customdataset(data_dir='chest_xray/train', transform=val_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
      "    RandomHorizontalFlip(p=0.5)\n",
      "    RandomRotation(degrees=[-10.0, 10.0], interpolation=nearest, expand=False, fill=0)\n",
      "    ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2), saturation=None, hue=None)\n",
      "    RandomAffine(degrees=[0.0, 0.0], translate=(0.1, 0.1))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.5], std=[0.5])\n",
      ") Compose(\n",
      "    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
      "    RandomHorizontalFlip(p=0.5)\n",
      "    RandomRotation(degrees=[-10.0, 10.0], interpolation=nearest, expand=False, fill=0)\n",
      "    ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2), saturation=None, hue=None)\n",
      "    RandomAffine(degrees=[0.0, 0.0], translate=(0.1, 0.1))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.5], std=[0.5])\n",
      ") Compose(\n",
      "    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
      "    Grayscale(num_output_channels=1)\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.5], std=[0.5])\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(full_dataset.transform, train_dataset.dataset.transform, val_dataset.dataset.transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader 설정\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedCNN(nn.Module): \n",
    "    def __init__(self): \n",
    "        #초기화, 변수 설정해주는 구간 \n",
    "        super(EnhancedCNN, self).__init__()\n",
    "        self.conv1= nn.Conv2d(1, 32, kernel_size= 3, stride= 1, padding= 1) \n",
    "        self.bn1= nn.BatchNorm2d(32) \n",
    "        self.conv2= nn.Conv2d(32, 64, kernel_size= 3, stride= 1, padding= 1) \n",
    "        self.bn2= nn.BatchNorm2d(64) \n",
    "        self.conv3= nn.Conv2d(64, 128, kernel_size= 3, stride= 1, padding= 1) \n",
    "        self.bn3= nn.BatchNorm2d(128)\n",
    "        self.pool= nn.MaxPool2d(kernel_size= 2, stride= 2, padding= 0) \n",
    "        self.dropout= nn.Dropout(0.5) \n",
    "        self.fc1= nn.Linear(128*28*28, 256)\n",
    "        self.fc2= nn.Linear(256, 2) \n",
    "\n",
    "    def forward(self, x): \n",
    "        #여기가 실제로 층 쌓는 구간\n",
    "        x= self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x= self.pool(F.relu(self.bn2(self.conv2(x)))) \n",
    "        x= self.pool(F.relu(self.bn3(self.conv3(x)))) \n",
    "        x= x.view(-1, 128*28*28)\n",
    "        x= F.relu(self.fc1(x)) \n",
    "        x= self.dropout(x) \n",
    "        x= self.fc2(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "device= torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model= EnhancedCNN().to(device) \n",
    "criterion= nn.CrossEntropyLoss() \n",
    "optimizer= optim.SGD(model.parameters(), lr= 0.001, momentum= 0.9)\n",
    "num_epochs= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.4242\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[191], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#recall 구하려고 하는거 예측값 저장\u001b[39;00m\n\u001b[1;32m     21\u001b[0m _, predicted\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m all_train_labels\u001b[38;5;241m.\u001b[39mextend(\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     23\u001b[0m all_train_predictions\u001b[38;5;241m.\u001b[39mextend(predicted\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()) \n\u001b[1;32m     25\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward() \n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs): \n",
    "    #초기화 해주는거 \n",
    "    #학습모드로 변경\n",
    "    model.train() \n",
    "    running_loss= 0.0\n",
    "\n",
    "    #얘네는 recall 구하기 위해서 추가한거\n",
    "    all_train_labels= []\n",
    "    all_train_predictions= [] \n",
    "\n",
    "    for images, labels in train_loader: \n",
    "        #gpu 디바이스로 적용해서 진행하려고 to(device) \n",
    "        images, labels= images.to(device), labels.to(device) \n",
    "\n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        outputs= model(images) \n",
    "        loss= criterion(outputs, labels) \n",
    "\n",
    "        #recall 구하려고 하는거 예측값 저장\n",
    "        _, predicted= torch.max(outputs.data, 1)\n",
    "        all_train_labels.extend(labels.cpu().numpy())\n",
    "        all_train_predictions.extend(predicted.cpu().numpy()) \n",
    "\n",
    "        loss.backward() \n",
    "        optimizer.step() \n",
    "\n",
    "        running_loss+= loss.item()\n",
    "    #손실 출력 \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}')\n",
    "\n",
    "    #recall 값 출력 \n",
    "    train_report= classification_report(all_train_labels, all_train_predictions, target_names= ['NORMAL', 'PNEUMONIA'], output_dict= True) \n",
    "    train_recall= train_report['weighted avg']['recall']\n",
    "    print(f'train_recall: {train_recall:.4f}')\n",
    "\n",
    "\n",
    "\n",
    "    #val 상단에 해놓은거랑 똑같이 하면 댐\n",
    "    model.eval() \n",
    "    all_val_labels= []\n",
    "    all_val_predictions= [] \n",
    "\n",
    "    #정확도 구할 때 씀 \n",
    "    correct= 0 \n",
    "    total= 0 \n",
    "\n",
    "    #\n",
    "    with torch.no_grad(): \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "module01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
