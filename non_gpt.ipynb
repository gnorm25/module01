{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import torchvision\n",
    "import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class customdataset(Dataset): \n",
    "    def __init__(self, data_dir, transform= None): \n",
    "        #이닛 해주고\n",
    "        self.data_dir= data_dir \n",
    "        self.transform= transform\n",
    "        #이미지들 경로명 담을 리스트\n",
    "        self.image_paths= []\n",
    "        self.labels= []\n",
    "\n",
    "        # NORMAL 폴더의 이미지 경로와 라벨 (0)\n",
    "        normal_dir = os.path.join(data_dir, 'NORMAL')\n",
    "        for img_name in os.listdir(normal_dir):\n",
    "            self.image_paths.append(os.path.join(normal_dir, img_name))\n",
    "            self.labels.append(0)\n",
    "\n",
    "        # PNEUMONIA 폴더의 이미지 경로와 라벨 (1)\n",
    "        pneumonia_dir = os.path.join(data_dir, 'PNEUMONIA')\n",
    "        for img_name in os.listdir(pneumonia_dir):\n",
    "            self.image_paths.append(os.path.join(pneumonia_dir, img_name))\n",
    "            self.labels.append(1)\n",
    "\n",
    "            \n",
    "    def __len__(self): \n",
    "        #len이건 필수래 나중에 DataLoader 하려면 \n",
    "        #몇번 반복해서 받을건지를 결정하는 요소? \n",
    "        #이미지파일 갯수만큼 받아야하니까\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        #이것도 필수래 DataLoader 에서 이터레이터한 객체를 받아올 수 있다는데? \n",
    "        #그래서 for문같은 반복문 추가 안해줘도 DataLoader에서 알아서 반복 해준대 ㅇㅇ\n",
    "        #이미지 경로를 받아서 실제 이미지를 열어주는 구간\n",
    "        \n",
    "        image = Image.open(self.image_paths[idx]).convert('L')\n",
    "        label= self.labels[idx]\n",
    "\n",
    "        if self.transform: \n",
    "            image = np.array(image)\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.Compose([\n",
    "    A.Resize(224, 224),  # 이미지 크기 조정\n",
    "    A.Rotate(limit=5),  # 최대 5도 회전\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),  # 밝기 및 대비 조절\n",
    "    A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=0, p=0.5),  # 약간의 이동 및 확대/축소\n",
    "    A.GaussNoise(var_limit=(10.0, 50.0), p=0.2),  # 약간의 가우시안 노이즈 추가\n",
    "    A.Normalize(mean=(0.5,), std=(0.5,)),  # 정규화\n",
    "    ToTensorV2()  # 텐서로 변환\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(224, 224),  # 검증 데이터에는 기본 전처리만 적용\n",
    "    A.Normalize(mean=(0.5,), std=(0.5,)),\n",
    "    ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset= customdataset(data_dir= 'chest_xray/train', transform= train_transform) \n",
    "train_size= int(0.8*len(full_dataset)) \n",
    "val_size= len(full_dataset) - train_size\n",
    "train_dataset, val_dataset= random_split(full_dataset, [train_size, val_size]) \n",
    "# 검증 데이터는 데이터 증강을 적용하지 않기 때문에 변환을 따로 지정\n",
    "val_dataset.dataset = customdataset(data_dir='chest_xray/train', transform=val_transform)\n",
    "# DataLoader 설정\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import torchvision\n",
    "import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 데이터셋 클래스 정의\n",
    "class customdataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        normal_dir = os.path.join(data_dir, 'NORMAL')\n",
    "        for img_name in os.listdir(normal_dir):\n",
    "            self.image_paths.append(os.path.join(normal_dir, img_name))\n",
    "            self.labels.append(0)\n",
    "\n",
    "        pneumonia_dir = os.path.join(data_dir, 'PNEUMONIA')\n",
    "        for img_name in os.listdir(pneumonia_dir):\n",
    "            self.image_paths.append(os.path.join(pneumonia_dir, img_name))\n",
    "            self.labels.append(1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert('L')\n",
    "        label = self.labels[idx]\n",
    "        return np.array(image), label  # PIL 이미지를 numpy 배열로 변환하여 반환\n",
    "\n",
    "# 원본 데이터셋 로드 (transform 없음)\n",
    "full_dataset = customdataset(data_dir='chest_xray/train')\n",
    "\n",
    "# 데이터셋 분할 (80% 훈련, 20% 검증)\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_indices, val_indices = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# Albumentations transform 정의\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(224, 224),  # 이미지 크기 조정\n",
    "    A.Rotate(limit=5),  # 최대 5도 회전\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),  # 밝기 및 대비 조절\n",
    "    A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=0, p=0.5),  # 약간의 이동 및 확대/축소\n",
    "    A.GaussNoise(var_limit=(10.0, 50.0), p=0.2),  # 약간의 가우시안 노이즈 추가\n",
    "    A.Normalize(mean=(0.5,), std=(0.5,)),  # 정규화\n",
    "    ToTensorV2()  # 텐서로 변환\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(224, 224),  # 검증 데이터에는 기본 전처리만 적용\n",
    "    A.Normalize(mean=(0.5,), std=(0.5,)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# transform을 적용한 새로운 데이터셋 클래스 정의\n",
    "class TransformedDataset(Dataset):\n",
    "    def __init__(self, dataset, indices, transform=None):\n",
    "        self.dataset = Subset(dataset, indices)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.dataset[idx]\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)  # Albumentations transform 적용\n",
    "            image = augmented['image']\n",
    "        return image, label\n",
    "\n",
    "# 각 데이터셋에 transform 적용\n",
    "train_dataset = TransformedDataset(full_dataset, train_indices.indices, transform=train_transform)\n",
    "val_dataset = TransformedDataset(full_dataset, val_indices.indices, transform=val_transform)\n",
    "\n",
    "# DataLoader 설정\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedCNN(nn.Module): \n",
    "    def __init__(self): \n",
    "        #초기화, 변수 설정해주는 구간 \n",
    "        super(EnhancedCNN, self).__init__()\n",
    "        self.conv1= nn.Conv2d(1, 32, kernel_size= 3, stride= 1, padding= 1) \n",
    "        self.bn1= nn.BatchNorm2d(32) \n",
    "        self.conv2= nn.Conv2d(32, 64, kernel_size= 3, stride= 1, padding= 1) \n",
    "        self.bn2= nn.BatchNorm2d(64) \n",
    "        self.conv3= nn.Conv2d(64, 128, kernel_size= 3, stride= 1, padding= 1) \n",
    "        self.bn3= nn.BatchNorm2d(128)\n",
    "        self.pool= nn.MaxPool2d(kernel_size= 2, stride= 2, padding= 0) \n",
    "        self.dropout= nn.Dropout(0.5) \n",
    "        self.fc1= nn.Linear(128*28*28, 256)\n",
    "        self.fc2= nn.Linear(256, 2) \n",
    "\n",
    "    def forward(self, x): \n",
    "        #여기가 실제로 층 쌓는 구간\n",
    "        x= self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x= self.pool(F.relu(self.bn2(self.conv2(x)))) \n",
    "        x= self.pool(F.relu(self.bn3(self.conv3(x)))) \n",
    "        x= x.view(-1, 128*28*28)\n",
    "        x= F.relu(self.fc1(x)) \n",
    "        x= self.dropout(x) \n",
    "        x= self.fc2(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "device= torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model= EnhancedCNN().to(device) \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "criterion= nn.CrossEntropyLoss()\n",
    "num_epochs= 50\n",
    "# 조기 종료 조건 설정\n",
    "patience = 7\n",
    "best_val_loss = float('inf')\n",
    "early_stop_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131/131 [00:15<00:00,  8.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50]\n",
      "Loss: 1.8216, Train Accuracy: 87.90%, recall: 0.8790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.1311, Valid Accuracy: 94.25%, recall: 0.9425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131/131 [00:15<00:00,  8.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50]\n",
      "Loss: 0.1757, Train Accuracy: 92.67%, recall: 0.9267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0864, Valid Accuracy: 96.26%, recall: 0.9626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131/131 [00:15<00:00,  8.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/50]\n",
      "Loss: 0.1797, Train Accuracy: 92.62%, recall: 0.9262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.1004, Valid Accuracy: 95.79%, recall: 0.9579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131/131 [00:16<00:00,  8.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/50]\n",
      "Loss: 0.1789, Train Accuracy: 93.43%, recall: 0.9343\n",
      "Loss: 0.1171, Valid Accuracy: 94.64%, recall: 0.9464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131/131 [00:16<00:00,  8.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/50]\n",
      "Loss: 0.1770, Train Accuracy: 93.26%, recall: 0.9326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0908, Valid Accuracy: 96.36%, recall: 0.9636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131/131 [00:16<00:00,  8.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/50]\n",
      "Loss: 0.1384, Train Accuracy: 94.75%, recall: 0.9475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0852, Valid Accuracy: 96.84%, recall: 0.9684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131/131 [00:16<00:00,  7.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/50]\n",
      "Loss: 0.1225, Train Accuracy: 95.42%, recall: 0.9542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0771, Valid Accuracy: 96.46%, recall: 0.9646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131/131 [00:17<00:00,  7.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/50]\n",
      "Loss: 0.1144, Train Accuracy: 95.54%, recall: 0.9554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0785, Valid Accuracy: 97.13%, recall: 0.9713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131/131 [00:16<00:00,  7.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/50]\n",
      "Loss: 0.1165, Train Accuracy: 95.71%, recall: 0.9571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0790, Valid Accuracy: 97.13%, recall: 0.9713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131/131 [00:17<00:00,  7.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/50]\n",
      "Loss: 0.1186, Train Accuracy: 96.48%, recall: 0.9648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0817, Valid Accuracy: 96.65%, recall: 0.9665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131/131 [00:17<00:00,  7.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/50]\n",
      "Loss: 0.1093, Train Accuracy: 96.16%, recall: 0.9616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0792, Valid Accuracy: 96.65%, recall: 0.9665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131/131 [00:16<00:00,  7.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/50]\n",
      "Loss: 0.1064, Train Accuracy: 96.69%, recall: 0.9669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0792, Valid Accuracy: 96.84%, recall: 0.9684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131/131 [00:18<00:00,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/50]\n",
      "Loss: 0.1048, Train Accuracy: 96.43%, recall: 0.9643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0790, Valid Accuracy: 96.74%, recall: 0.9674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131/131 [00:18<00:00,  6.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/50]\n",
      "Loss: 0.0970, Train Accuracy: 96.69%, recall: 0.9669\n",
      "Loss: 0.0789, Valid Accuracy: 96.84%, recall: 0.9684\n",
      "Early stopping at epoch 14\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs): \n",
    "    #초기화 해주는거 \n",
    "    #학습모드로 변경\n",
    "    model.train() \n",
    "    running_loss= 0.0\n",
    "    #얘네는 recall 구하기 위해서 추가한거\n",
    "    all_train_labels= []\n",
    "    all_train_predictions= [] \n",
    "    #정확도 구할 때 씀 \n",
    "    correct= 0 \n",
    "    total= 0 \n",
    "\n",
    "    for images, labels in tqdm.tqdm(train_loader): \n",
    "        #gpu 디바이스로 적용해서 진행하려고 to(device) \n",
    "        images, labels= images.to(device), labels.to(device) \n",
    "\n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        outputs= model(images) \n",
    "        loss= criterion(outputs, labels) \n",
    "\n",
    "        #recall 구하려고 하는거 예측값 저장\n",
    "        _, predicted= torch.max(outputs.data, 1)\n",
    "        all_train_labels.extend(labels.cpu().numpy())\n",
    "        all_train_predictions.extend(predicted.cpu().numpy()) \n",
    "\n",
    "        #정확도 구하자\n",
    "        total+= labels.size(0) \n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        loss.backward() \n",
    "        optimizer.step() \n",
    "\n",
    "        running_loss+= loss.item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    #recall 값 출력 \n",
    "    train_report= classification_report(all_train_labels, all_train_predictions, target_names= ['NORMAL', 'PNEUMONIA'], output_dict= True) \n",
    "    train_recall= train_report['weighted avg']['recall']\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "    print(f'Loss: {running_loss / len(train_loader):.4f}, Train Accuracy: {accuracy:.2f}%, recall: {train_recall:.4f}')\n",
    "\n",
    "    #val 상단에 해놓은거랑 똑같이 하면 댐\n",
    "    model.eval() \n",
    "    val_loss = 0.0\n",
    "    all_val_labels= []\n",
    "    all_val_predictions= [] \n",
    "    \n",
    "    correct= 0\n",
    "    total= 0 \n",
    "\n",
    "    #검증용이라서 역전파 없이 함\n",
    "    with torch.no_grad(): \n",
    "        for images, labels in val_loader: \n",
    "            images, labels= images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs= model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            #recall 구하려고 하는거 예측값 저장\n",
    "            _, predicted= torch.max(outputs.data, 1) \n",
    "            all_val_labels.extend(labels.cpu().numpy())\n",
    "            all_val_predictions.extend(predicted.cpu().numpy())\n",
    "    \n",
    "            #정확도 구하자\n",
    "            total+= labels.size(0) \n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # 검증 데이터 예측 지표 출력\n",
    "    val_report = classification_report(all_val_labels, all_val_predictions, target_names=['NORMAL', 'PNEUMONIA'], output_dict=True)\n",
    "    val_weighted_avg_recall = val_report['weighted avg']['recall']\n",
    "\n",
    "    # 정확도 출력\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Loss: {val_loss / len(val_loader):.4f}, Valid Accuracy: {accuracy:.2f}%, recall: {val_weighted_avg_recall:.4f}')\n",
    "\n",
    "    # 학습률 스케줄러 스텝\n",
    "    scheduler.step()\n",
    "\n",
    "    # 조기 종료(Early Stopping) 체크\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        early_stop_counter = 0\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        if early_stop_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      NORMAL       0.97      0.48      0.65       242\n",
      "   PNEUMONIA       0.76      0.99      0.86       398\n",
      "\n",
      "    accuracy                           0.80       640\n",
      "   macro avg       0.87      0.74      0.75       640\n",
      "weighted avg       0.84      0.80      0.78       640\n",
      "\n",
      "Weighted Average Recall: 0.8000\n",
      "Accuracy: 80.0000%\n"
     ]
    }
   ],
   "source": [
    "test_transform = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.Normalize(mean=(0.5,), std=(0.5,)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "test_= customdataset(data_dir='chest_xray/test')\n",
    "test_dataset = TransformedDataset(test_, range(len(test_)), transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "# 모델 평가 모드로 설정\n",
    "model.eval()\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "total= 0\n",
    "correct= 0\n",
    "\n",
    "# 테스트 데이터셋에 대한 예측 및 실제 라벨 저장\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        \n",
    "        # recall 구하려고 실제 라벨과 예측 라벨을 리스트에 추가\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "        #정확도 구하자\n",
    "        total+= labels.size(0) \n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "# Classification Report 출력 및 Weighted Average Recall 추출\n",
    "report = classification_report(all_labels, all_predictions, target_names=['NORMAL', 'PNEUMONIA'], output_dict=True)\n",
    "\n",
    "# 출력 보고서에서 weighted avg recall 값 추출\n",
    "weighted_avg_recall = report['weighted avg']['recall']\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_labels, all_predictions, target_names=['NORMAL', 'PNEUMONIA']))\n",
    "print(f\"Weighted Average Recall: {weighted_avg_recall:.4f}\")\n",
    "\n",
    "# 정확도 출력\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy: {accuracy:.4f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "module01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
