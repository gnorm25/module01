{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import torchvision\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "class customdataset(Dataset): \n",
    "    def __init__(self, data_dir, transform= None): \n",
    "        #이닛 해주고\n",
    "        self.data_dir= data_dir \n",
    "        self.transform= transform\n",
    "        #이미지들 경로명 담을 리스트\n",
    "        self.image_paths= []\n",
    "        self.labels= []\n",
    "\n",
    "        # NORMAL 폴더의 이미지 경로와 라벨 (0)\n",
    "        normal_dir = os.path.join(data_dir, 'NORMAL')\n",
    "        for img_name in os.listdir(normal_dir):\n",
    "            self.image_paths.append(os.path.join(normal_dir, img_name))\n",
    "            self.labels.append(0)\n",
    "\n",
    "        # PNEUMONIA 폴더의 이미지 경로와 라벨 (1)\n",
    "        pneumonia_dir = os.path.join(data_dir, 'PNEUMONIA')\n",
    "        for img_name in os.listdir(pneumonia_dir):\n",
    "            self.image_paths.append(os.path.join(pneumonia_dir, img_name))\n",
    "            self.labels.append(1)\n",
    "\n",
    "            \n",
    "    def __len__(self): \n",
    "        #len이건 필수래 나중에 DataLoader 하려면 \n",
    "        #몇번 반복해서 받을건지를 결정하는 요소? \n",
    "        #이미지파일 갯수만큼 받아야하니까\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        #이것도 필수래 DataLoader 에서 이터레이터한 객체를 받아올 수 있다는데? \n",
    "        #그래서 for문같은 반복문 추가 안해줘도 DataLoader에서 알아서 반복 해준대 ㅇㅇ\n",
    "        #이미지 경로를 받아서 실제 이미지를 열어주는 구간\n",
    "        \n",
    "        image = Image.open(self.image_paths[idx]).convert('L')\n",
    "        label= self.labels[idx]\n",
    "\n",
    "        if self.transform: \n",
    "            image= self.transform(image)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform= transforms.Compose([ \n",
    "    #데이터 전처리 하기위해서 필수인 과정 \n",
    "    #데이터 증강도 할 수 있음 +a인 느낌 \n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),  # 수평 뒤집기\n",
    "    transforms.RandomVerticalFlip(),    # 수직 뒤집기 추가\n",
    "    transforms.RandomRotation(30),      # 더 강한 회전 추가\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),  # 더 강한 색상 변형\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.2, 0.2)),  # 더 강한 이동 변환\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "val_transform= transforms.Compose([\n",
    "    #검증 데이터셋을 만들 땐 전처리만\n",
    "    #데이터 증강은 필요없음. \n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Grayscale(num_output_channels=1), \n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize(mean=[0.5], std=[0.5]) \n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset= customdataset(data_dir= 'chest_xray/train', transform= train_transform) \n",
    "train_size= int(0.8*len(full_dataset)) \n",
    "val_size= len(full_dataset) - train_size\n",
    "train_dataset, val_dataset= random_split(full_dataset, [train_size, val_size]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5216"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
      "    RandomHorizontalFlip(p=0.5)\n",
      "    RandomRotation(degrees=[-10.0, 10.0], interpolation=nearest, expand=False, fill=0)\n",
      "    ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2), saturation=None, hue=None)\n",
      "    RandomAffine(degrees=[0.0, 0.0], translate=(0.1, 0.1))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.5], std=[0.5])\n",
      ") Compose(\n",
      "    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
      "    RandomHorizontalFlip(p=0.5)\n",
      "    RandomRotation(degrees=[-10.0, 10.0], interpolation=nearest, expand=False, fill=0)\n",
      "    ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2), saturation=None, hue=None)\n",
      "    RandomAffine(degrees=[0.0, 0.0], translate=(0.1, 0.1))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.5], std=[0.5])\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(full_dataset.transform, train_dataset.dataset.transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 검증 데이터는 데이터 증강을 적용하지 않기 때문에 변환을 따로 지정\n",
    "val_dataset.dataset = customdataset(data_dir='chest_xray/train', transform=val_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
      "    RandomHorizontalFlip(p=0.5)\n",
      "    RandomRotation(degrees=[-10.0, 10.0], interpolation=nearest, expand=False, fill=0)\n",
      "    ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2), saturation=None, hue=None)\n",
      "    RandomAffine(degrees=[0.0, 0.0], translate=(0.1, 0.1))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.5], std=[0.5])\n",
      ") Compose(\n",
      "    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
      "    RandomHorizontalFlip(p=0.5)\n",
      "    RandomRotation(degrees=[-10.0, 10.0], interpolation=nearest, expand=False, fill=0)\n",
      "    ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2), saturation=None, hue=None)\n",
      "    RandomAffine(degrees=[0.0, 0.0], translate=(0.1, 0.1))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.5], std=[0.5])\n",
      ") Compose(\n",
      "    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
      "    Grayscale(num_output_channels=1)\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.5], std=[0.5])\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(full_dataset.transform, train_dataset.dataset.transform, val_dataset.dataset.transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader 설정\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.downsample = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.downsample(x)\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class EnhancedResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EnhancedResNet, self).__init__()\n",
    "        self.layer1 = ResidualBlock(1, 32)\n",
    "        self.layer2 = ResidualBlock(32, 64, stride=2)\n",
    "        self.layer3 = ResidualBlock(64, 128, stride=2)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(128, 2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedCNN(nn.Module): \n",
    "    def __init__(self): \n",
    "        #초기화, 변수 설정해주는 구간 \n",
    "        super(EnhancedCNN, self).__init__()\n",
    "        self.conv1= nn.Conv2d(1, 32, kernel_size= 3, stride= 1, padding= 1) \n",
    "        self.bn1= nn.BatchNorm2d(32) \n",
    "        self.conv2= nn.Conv2d(32, 64, kernel_size= 3, stride= 1, padding= 1) \n",
    "        self.bn2= nn.BatchNorm2d(64) \n",
    "        self.conv3= nn.Conv2d(64, 128, kernel_size= 3, stride= 1, padding= 1) \n",
    "        self.bn3= nn.BatchNorm2d(128)\n",
    "        self.pool= nn.MaxPool2d(kernel_size= 2, stride= 2, padding= 0) \n",
    "        self.dropout= nn.Dropout(0.5) \n",
    "        self.fc1= nn.Linear(128*28*28, 256)\n",
    "        self.fc2= nn.Linear(256, 2) \n",
    "\n",
    "    def forward(self, x): \n",
    "        #여기가 실제로 층 쌓는 구간\n",
    "        x= self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x= self.pool(F.relu(self.bn2(self.conv2(x)))) \n",
    "        x= self.pool(F.relu(self.bn3(self.conv3(x)))) \n",
    "        x= x.view(-1, 128*28*28)\n",
    "        x= F.relu(self.fc1(x)) \n",
    "        x= self.dropout(x) \n",
    "        x= self.fc2(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "device= torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model= EnhancedResNet().to(device) \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "criterion= nn.CrossEntropyLoss()\n",
    "num_epochs= 50\n",
    "# 조기 종료 조건 설정\n",
    "patience = 5\n",
    "best_val_loss = float('inf')\n",
    "early_stop_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.4032, Train Accuracy: 81.30%\n",
      "train_recall: 0.8130\n",
      "val_recall: 0.9023\n",
      "Loss: 1.6007, Validation Accuracy: 90.23%\n",
      "Epoch [2/10], Loss: 0.2966, Train Accuracy: 87.37%\n",
      "train_recall: 0.8737\n",
      "val_recall: 0.9310\n",
      "Loss: 1.1775, Validation Accuracy: 93.10%\n",
      "Epoch [3/10], Loss: 0.2422, Train Accuracy: 89.98%\n",
      "train_recall: 0.8998\n",
      "val_recall: 0.9301\n",
      "Loss: 0.9613, Validation Accuracy: 93.01%\n",
      "Epoch [4/10], Loss: 0.2178, Train Accuracy: 91.51%\n",
      "train_recall: 0.9151\n",
      "val_recall: 0.9540\n",
      "Loss: 0.8645, Validation Accuracy: 95.40%\n",
      "Epoch [5/10], Loss: 0.2248, Train Accuracy: 91.08%\n",
      "train_recall: 0.9108\n",
      "val_recall: 0.9157\n",
      "Loss: 0.8924, Validation Accuracy: 91.57%\n",
      "Epoch [6/10], Loss: 0.1904, Train Accuracy: 92.69%\n",
      "train_recall: 0.9269\n",
      "val_recall: 0.8812\n",
      "Loss: 0.7558, Validation Accuracy: 88.12%\n",
      "Epoch [7/10], Loss: 0.2033, Train Accuracy: 91.73%\n",
      "train_recall: 0.9173\n",
      "val_recall: 0.9473\n",
      "Loss: 0.8072, Validation Accuracy: 94.73%\n",
      "Epoch [8/10], Loss: 0.1934, Train Accuracy: 93.05%\n",
      "train_recall: 0.9305\n",
      "val_recall: 0.9674\n",
      "Loss: 0.7678, Validation Accuracy: 96.74%\n",
      "Epoch [9/10], Loss: 0.1978, Train Accuracy: 92.71%\n",
      "train_recall: 0.9271\n",
      "val_recall: 0.9569\n",
      "Loss: 0.7853, Validation Accuracy: 95.69%\n",
      "Epoch [10/10], Loss: 0.1893, Train Accuracy: 92.67%\n",
      "train_recall: 0.9267\n",
      "val_recall: 0.8669\n",
      "Loss: 0.7515, Validation Accuracy: 86.69%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs): \n",
    "    #초기화 해주는거 \n",
    "    #학습모드로 변경\n",
    "    model.train() \n",
    "    running_loss= 0.0\n",
    "\n",
    "    #얘네는 recall 구하기 위해서 추가한거\n",
    "    all_train_labels= []\n",
    "    all_train_predictions= [] \n",
    "    #정확도 구할 때 씀 \n",
    "    correct= 0 \n",
    "    total= 0 \n",
    "\n",
    "\n",
    "    for images, labels in tqdm.tqdm(train_loader): \n",
    "        #gpu 디바이스로 적용해서 진행하려고 to(device) \n",
    "        images, labels= images.to(device), labels.to(device) \n",
    "\n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        outputs= model(images) \n",
    "        loss= criterion(outputs, labels) \n",
    "\n",
    "        #recall 구하려고 하는거 예측값 저장\n",
    "        _, predicted= torch.max(outputs.data, 1)\n",
    "        all_train_labels.extend(labels.cpu().numpy())\n",
    "        all_train_predictions.extend(predicted.cpu().numpy()) \n",
    "\n",
    "        #정확도 구하자\n",
    "        total+= labels.size(0) \n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        loss.backward() \n",
    "        optimizer.step() \n",
    "\n",
    "        running_loss+= loss.item()\n",
    "    #손실 출력 \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}, Train Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "    #recall 값 출력 \n",
    "    train_report= classification_report(all_train_labels, all_train_predictions, target_names= ['NORMAL', 'PNEUMONIA'], output_dict= True) \n",
    "    train_recall= train_report['weighted avg']['recall']\n",
    "    print(f'train_recall: {train_recall:.4f}')\n",
    "\n",
    "\n",
    "\n",
    "    #val 상단에 해놓은거랑 똑같이 하면 댐\n",
    "    model.eval() \n",
    "    val_loss = 0.0\n",
    "    all_val_labels= []\n",
    "    all_val_predictions= [] \n",
    "    \n",
    "\n",
    "    correct= 0 \n",
    "    total= 0 \n",
    "\n",
    "    #검증용이라서 역전파 없이 함\n",
    "    with torch.no_grad(): \n",
    "        for images, labels in val_loader: \n",
    "            images, labels= images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs= model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "\n",
    "            #recall 구하려고 하는거 예측값 저장\n",
    "            _, predicted= torch.max(outputs.data, 1) \n",
    "            all_val_labels.extend(labels.cpu().numpy())\n",
    "            all_val_predictions.extend(predicted.cpu().numpy())\n",
    "    \n",
    "            #정확도 구하자\n",
    "            total+= labels.size(0) \n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # 검증 데이터 예측 지표 출력\n",
    "    val_report = classification_report(all_val_labels, all_val_predictions, target_names=['NORMAL', 'PNEUMONIA'], output_dict=True)\n",
    "    val_weighted_avg_recall = val_report['weighted avg']['recall']\n",
    "    print(f\"val_recall: {val_weighted_avg_recall:.4f}\")\n",
    "\n",
    "    # 정확도 출력\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Loss: {val_loss / len(val_loader):.4f}, Validation Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "    # 학습률 스케줄러 스텝\n",
    "    scheduler.step()\n",
    "\n",
    "    # 조기 종료(Early Stopping) 체크\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        early_stop_counter = 0\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        if early_stop_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      NORMAL       0.80      0.86      0.83       242\n",
      "   PNEUMONIA       0.91      0.87      0.89       398\n",
      "\n",
      "    accuracy                           0.87       640\n",
      "   macro avg       0.86      0.86      0.86       640\n",
      "weighted avg       0.87      0.87      0.87       640\n",
      "\n",
      "Weighted Average Recall: 0.8656\n",
      "Accuracy: 86.5625%\n"
     ]
    }
   ],
   "source": [
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "test_dataset = customdataset(data_dir='chest_xray/test', transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "# 모델 평가 모드로 설정\n",
    "model.eval()\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "total= 0\n",
    "correct= 0\n",
    "\n",
    "# 테스트 데이터셋에 대한 예측 및 실제 라벨 저장\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        \n",
    "        outputs = model(images)\n",
    "        \n",
    "        \n",
    "        # recall 구하려고 실제 라벨과 예측 라벨을 리스트에 추가\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "\n",
    "        #정확도 구하자\n",
    "        total+= labels.size(0) \n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "# Classification Report 출력 및 Weighted Average Recall 추출\n",
    "report = classification_report(all_labels, all_predictions, target_names=['NORMAL', 'PNEUMONIA'], output_dict=True)\n",
    "\n",
    "# 출력 보고서에서 weighted avg recall 값 추출\n",
    "weighted_avg_recall = report['weighted avg']['recall']\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_labels, all_predictions, target_names=['NORMAL', 'PNEUMONIA']))\n",
    "print(f\"Weighted Average Recall: {weighted_avg_recall:.4f}\")\n",
    "\n",
    "# 정확도 출력\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy: {accuracy:.4f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "module01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
